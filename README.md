# GPT-2 Text Generator - Fine-tuned on Custom Dataset

This project demonstrates how to fine-tune OpenAI's GPT-2 language model using HuggingFace Transformers on a custom text dataset.

## ğŸ“š Description

Using HuggingFace's `transformers` library, this script fine-tunes GPT-2 on a small, custom dataset to generate creative text. You can adapt this to fine-tune GPT-2 for poetry, stories, chatbots, or domain-specific language tasks.

## ğŸš€ Features

- Fine-tunes GPT-2 on your own `.txt` file
- Uses HuggingFace `Trainer` and `datasets` for efficient training
- Saves model for later text generation
- Easily extensible for larger datasets

## ğŸ“ Dataset

The model was trained on a small custom dataset of 5 text samples:
